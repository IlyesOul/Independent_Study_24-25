import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import utils
from tensorflow.keras.optimizers import Adam


# Loading data
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()

# Train-test split + scaling
x_train = train_x[:int(.7*len(train_x))]/255.0
validate_x = train_x[int(.7*len(train_x)):]/255.0
test_x = test_x/255.0

y_train = train_y[:int(.7*len(train_y))]
validate_y = train_y[int(.7*len(train_y)):]

# One-hot encoding
train_y = utils.to_categorical(train_y)
validate_y = utils.to_categorical(validate_y)
test_y = utils.to_categorical(test_y)

# Visualize data for self
# for image in train_x:
#     plt.imshow(image)
#     plt.show()

num_filters = 16  # Number of filters (proportional to how many feature maps are created)
kernel_size = (2, 2)  # Size of kernel/filter
pooling_size = (5, 5)  # Max pooling size

cnn = models.Sequential()

# Experiment one: number of filters
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=kernel_size))  # 16 Filters
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=kernel_size))  # 32 Filters
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=kernel_size))  # 8 Filters
cnn.add(tf.keras.layers.MaxPooling2D(pool_size=pooling_size))
cnn.add(tf.keras.layers.Flatten())
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=10, activation="softmax"))

adam = Adam(learning_rate=.001)
cnn.compile(optimizer=adam, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
kernel_history = cnn.fit(x=train_x, y=train_y, validation_data=(validate_x, validate_y), verbose='auto', epochs=50)
predicted_values = cnn.predict(test_x)

print(f"Prediction loss: {tf.keras.losses.categorical_crossentropy(test_y, predicted_values)}")
print(f"true value: {test_y}")
print(f"loss history: {kernel_history.history['loss']}")

sixteen_filter_loss = [3.0221288204193115, 1.5294594764709473, 1.4163355827331543, 1.32173752784729, 1.2598520517349243, 1.2115743160247803, 1.1662541627883911, 1.1267049312591553, 1.0901415348052979, 1.063145637512207, 1.0383961200714111, 1.0174990892410278, 0.9909449219703674, 0.9760822057723999, 0.9613591432571411, 0.9415706992149353, 0.9295029044151306, 0.9116414189338684, 0.9032245874404907, 0.8905277848243713, 0.8732839226722717, 0.8647357821464539, 0.8523615002632141, 0.8474959135055542, 0.8338671922683716, 0.8237895965576172, 0.8130013346672058, 0.8010404706001282, 0.8022156357765198, 0.7919549345970154, 0.778273344039917, 0.7645033597946167, 0.755082368850708, 0.7473039627075195, 0.7421103119850159, 0.7328968644142151, 0.7379632592201233, 0.7203224301338196, 0.7081281542778015, 0.7080775499343872, 0.7013481855392456, 0.6936007142066956, 0.6995744705200195, 0.6815229654312134, 0.6769161224365234, 0.6795443892478943, 0.6665043830871582, 0.6712137460708618, 0.6688309907913208, 0.6440631151199341]
thirty_two_filter_loss = [2.5357344150543213, 1.5113455057144165, 1.3797639608383179, 1.2912317514419556, 1.2256274223327637, 1.1683480739593506, 1.129489779472351, 1.0958812236785889, 1.0654915571212769, 1.0447731018066406, 1.018872857093811, 0.9968739151954651, 0.9750971794128418, 0.9614270329475403, 0.9401007294654846, 0.9286236763000488, 0.9061633348464966, 0.8956688046455383, 0.8829875588417053, 0.873674213886261, 0.8516087532043457, 0.8462439179420471, 0.8338744044303894, 0.8230777978897095, 0.8141664862632751, 0.8024669289588928, 0.7915056347846985, 0.7850292325019836, 0.7722366452217102, 0.7794349789619446, 0.7473431825637817, 0.7528351545333862, 0.7446939945220947, 0.7286399006843567, 0.7291895151138306, 0.7262896299362183, 0.706271231174469, 0.7076407074928284, 0.6948656439781189, 0.695787787437439, 0.6781450510025024, 0.6820781230926514, 0.674811065196991, 0.6716402173042297, 0.6599913239479065, 0.6557605266571045, 0.658646821975708, 0.6455133557319641, 0.6505985260009766, 0.6345973610877991]
eight_filter_loss = [2.3368678092956543, 1.4262721538543701, 1.312034249305725, 1.2362686395645142, 1.169177532196045, 1.1231725215911865, 1.0787465572357178, 1.0394535064697266, 1.0086817741394043, 0.980991542339325, 0.9442100524902344, 0.9264071583747864, 0.9041051268577576, 0.8858774304389954, 0.8522727489471436, 0.8385986685752869, 0.8164583444595337, 0.8082449436187744, 0.7879174947738647, 0.7717483043670654, 0.7603763341903687, 0.7441768646240234, 0.7322308421134949, 0.7185478210449219, 0.7043806910514832, 0.6978471875190735, 0.677880048751831, 0.6736523509025574, 0.6772069334983826, 0.6474593877792358, 0.6583430171012878, 0.6331349611282349, 0.6320974826812744, 0.6242407560348511, 0.6324207186698914, 0.6113948225975037, 0.5881598591804504, 0.6013292670249939, 0.5833845734596252, 0.5792282223701477, 0.5763235092163086, 0.5640063881874084, 0.5717604756355286, 0.5638707280158997, 0.5468275547027588, 0.541573166847229, 0.5476768612861633, 0.5441411137580872, 0.5295628905296326, 0.5147902369499207]

# Visualize filter experiment

epochs = len(sixteen_filter_loss)
plt.plot(range(epochs), eight_filter_loss)
plt.plot(range(epochs), sixteen_filter_loss)
plt.plot(range(epochs), thirty_two_filter_loss)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["8 filters", "16 filters", "32 filters"])
plt.show()

callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]

# Experiment two: size of kernels
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=(1, 1)))  # (1x1)
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=(3, 3)))  # (3x3)
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=(7, 7)))  # (7x7)
cnn.add(tf.keras.layers.Conv2D(filters=16, strides=(1, 1), kernel_size=(12, 12)))  # (12x12)
cnn.add(tf.keras.layers.MaxPooling2D(pool_size=pooling_size))
cnn.add(tf.keras.layers.Flatten())
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=10, activation="softmax"))

adam = Adam(learning_rate=.001)
cnn.compile(optimizer=adam, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
kernel_history = cnn.fit(x=train_x, y=train_y, validation_data=(validate_x, validate_y), verbose='auto', epochs=50)
predicted_values = cnn.predict(test_x)

print(f"Prediction loss: {tf.keras.losses.categorical_crossentropy(test_y, predicted_values)}")
print(f"true value: {test_y}")
print(f"loss history: {kernel_history.history['loss']}")

# Visualize filter experiment

threes_matrix_loss = [2.5524418354034424, 1.6057052612304688, 1.4570136070251465, 1.3490681648254395, 1.274491310119629, 1.2116317749023438, 1.1729949712753296, 1.1384854316711426, 1.1062613725662231, 1.0749754905700684, 1.0539413690567017, 1.0277924537658691, 1.0090763568878174, 0.9848818778991699, 0.9702362418174744, 0.9556556344032288, 0.9350702166557312, 0.9311442375183105, 0.9189302325248718, 0.9005680680274963, 0.8887633085250854, 0.8734034895896912, 0.8610759973526001, 0.849642813205719, 0.8407008647918701, 0.8343029022216797, 0.8205395340919495, 0.8150807619094849, 0.8100237250328064, 0.7949643135070801, 0.7873182892799377, 0.7837856411933899, 0.76821368932724, 0.7719772458076477, 0.7610033750534058, 0.7511643767356873, 0.7517206072807312, 0.7454302310943604, 0.7435067892074585, 0.736530601978302, 0.7325387597084045, 0.7158557772636414, 0.7192634344100952, 0.7002978324890137, 0.6992488503456116, 0.6904982328414917, 0.6966114044189453, 0.6873267889022827, 0.6959542036056519, 0.6868358850479126]
sevens_matrix_loss = [2.4844894409179688, 1.6824674606323242, 1.546045184135437, 1.4387245178222656, 1.3559309244155884, 1.3050247430801392, 1.2597466707229614, 1.2493518590927124, 1.1951979398727417, 1.1874741315841675, 1.1672701835632324, 1.1521869897842407, 1.1349961757659912, 1.12867271900177, 1.1193708181381226, 1.1043540239334106, 1.0961108207702637, 1.093727469444275, 1.087169885635376, 1.0865317583084106, 1.0760353803634644, 1.0777864456176758, 1.0587104558944702, 1.0588433742523193, 1.0528607368469238, 1.0573365688323975, 1.0465106964111328, 1.0386877059936523, 1.0302001237869263, 1.0417227745056152, 1.0276316404342651, 1.020959496498108, 1.0200090408325195, 1.015264868736267, 1.014125943183899, 1.0154829025268555, 1.0095816850662231, 1.0020759105682373, 0.9983372092247009, 0.9981232285499573, 1.0042375326156616, 0.9959292411804199, 0.9928581118583679, 0.9913743734359741, 0.9923086762428284, 0.9825583696365356, 0.9747344255447388, 0.9707808494567871, 0.9836578369140625, 0.9634000062942505]
twelves_matrix_loss = [3.078845262527466, 1.9117549657821655, 1.9207396507263184, 1.8609533309936523, 1.8579367399215698, 1.8786590099334717, 1.8742287158966064, 1.840268611907959, 1.8290553092956543, 1.7962417602539062, 1.8098206520080566, 1.8487287759780884, 1.810113787651062, 1.82283353805542, 1.7914687395095825, 1.8251237869262695, 1.8383196592330933, 1.8753104209899902, 1.8142136335372925, 1.7722673416137695, 1.8298062086105347, 1.7966259717941284, 1.887635588645935, 2.0013649463653564, 1.9623697996139526, 2.4275898933410645, 2.3058066368103027, 2.302790880203247, 2.3027446269989014, 2.3027400970458984, 2.302769422531128, 2.302978515625, 2.3027875423431396, 2.3028085231781006, 2.3027660846710205, 2.3027970790863037, 2.3027663230895996, 2.3027846813201904, 2.3027422428131104, 2.302772045135498, 2.302762269973755, 2.30279541015625, 2.302785634994507, 2.302788734436035, 2.3027873039245605, 2.302800416946411, 2.3027758598327637, 2.302790641784668, 2.3027758598327637, 2.3027632236480713]


plt.plot(eight_filter_loss, label="1x1 Matrix")
plt.plot(threes_matrix_loss, label="3x3 Matrix")
plt.plot(sevens_matrix_loss, label="7x7 Matrix")
plt.plot(twelves_matrix_loss, label="12x12 Matrix")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs Epochs")
plt.legend()
plt.show()

# Experiment three: pooling matrix size

cnn.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1,1)))
cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))  # (2x2)
cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(5, 5)))  # (5x5)
cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(10, 10)))  # (10x10)
cnn.add(tf.keras.layers.Flatten())
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=150, activation="relu"))
cnn.add(tf.keras.layers.Dense(units=10, activation="softmax"))

adam = Adam(learning_rate=.001)
cnn.compile(optimizer=adam, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
kernel_history = cnn.fit(x=train_x, y=train_y, validation_data=(validate_x, validate_y), verbose='auto', epochs=50)

print(f"Loss History: {kernel_history.history['loss']}")

twos_matrix_loss = [4.088298320770264, 1.6282151937484741, 1.5014601945877075, 1.4327982664108276, 1.3563735485076904, 1.3061718940734863, 1.2376158237457275, 1.1778416633605957, 1.156078815460205, 1.1182376146316528, 1.0796786546707153, 1.058050513267517, 1.0420358180999756, 1.0005481243133545, 0.9854803085327148, 0.9796334505081177, 0.9459536075592041, 0.9308725595474243, 0.9178764224052429, 0.883289635181427, 0.8709540367126465, 0.8632091283798218, 0.8371357321739197, 0.8375445604324341, 0.8132404685020447, 0.8002955317497253, 0.7796437740325928, 0.7914229035377502, 0.7552824020385742, 0.7568500638008118, 0.7513917088508606, 0.7292822003364563, 0.7094568014144897, 0.7189953923225403, 0.6941139101982117, 0.6984744668006897, 0.6799800992012024, 0.7044482231140137, 0.6643127202987671, 0.6458176374435425, 0.6473335027694702, 0.6457518935203552, 0.6353435516357422, 0.6396666765213013, 0.6284731030464172, 0.6274389624595642, 0.5936305522918701, 0.5842441916465759, 0.6028438806533813, 0.5791088342666626]
fives_matrix_loss = [2.661233425140381, 1.5371019840240479, 1.4022867679595947, 1.3116230964660645, 1.2398713827133179, 1.182567834854126, 1.1392452716827393, 1.1011674404144287, 1.064988613128662, 1.031669020652771, 1.0045424699783325, 0.9814518094062805, 0.9536060690879822, 0.9386367797851562, 0.912186324596405, 0.8933373689651489, 0.877739667892456, 0.8665581345558167, 0.8460219502449036, 0.8329125046730042, 0.8207863569259644, 0.8056771755218506, 0.7956731915473938, 0.7934942841529846, 0.764477014541626, 0.7634684443473816, 0.7505156397819519, 0.7482486963272095, 0.7396692037582397, 0.7440359592437744, 0.7227191925048828, 0.7052094340324402, 0.7029228210449219, 0.6984947919845581, 0.6892708539962769, 0.6838975548744202, 0.6730974316596985, 0.6659821271896362, 0.6686010956764221, 0.659437894821167, 0.6577528715133667, 0.6479148268699646, 0.645180881023407, 0.6476055979728699, 0.624334454536438, 0.6341550946235657, 0.6352810263633728, 0.6303392052650452, 0.6259625554084778, 0.6220037341117859]
tens_matrix_loss = [3.2588391304016113, 1.5991058349609375, 1.4800857305526733, 1.405158281326294, 1.342151403427124, 1.2917152643203735, 1.2509678602218628, 1.2196255922317505, 1.1939440965652466, 1.1747469902038574, 1.148837924003601, 1.1355360746383667, 1.109941005706787, 1.1021044254302979, 1.082039475440979, 1.0712205171585083, 1.0578795671463013, 1.0457109212875366, 1.033039927482605, 1.022350549697876, 1.0201127529144287, 1.0052980184555054, 0.9912604689598083, 0.9953107833862305, 0.983052670955658, 0.9739562273025513, 0.963399350643158, 0.9580588340759277, 0.9561145901679993, 0.9512916207313538, 0.9440639615058899, 0.9328657984733582, 0.9330300092697144, 0.930826723575592, 0.917132556438446, 0.9128820300102234, 0.9047723412513733, 0.9098497629165649, 0.8956611752510071, 0.8945910930633545, 0.8903129696846008, 0.8862869739532471, 0.8892912268638611, 0.885247528553009, 0.8800424933433533, 0.8734539151191711, 0.8714500069618225, 0.866123616695404, 0.8614243865013123, 0.856265127658844]

plt.plot(twos_matrix_loss, label="2x2 Pool")
plt.plot(fives_matrix_loss, label="5x5 Pool")
plt.plot(tens_matrix_loss, label="10x10 Pool")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss per pooling size")
plt.legend()
plt.plot()
plt.show()
